{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 -  Data Sourcing via Web\n",
    "## Segment 5 - Introduction to NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"On Wednesday, the Association for Computing Machinery, the world’s largest society of computing professionals, announced that Hinton, LeCun and Bengio had won this year’s Turing Award for their work on neural networks. The Turing Award, which was introduced in 1966, is often called the Nobel Prize of computing, and it includes a $1 million prize, which the three scientists will share.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wayla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')#sentence tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sentence Tokenizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokenizing the text: \n",
      "\n",
      "['On Wednesday, the Association for Computing Machinery, the world’s largest society of computing professionals, announced that Hinton, LeCun and Bengio had won this year’s Turing Award for their work on neural networks.', 'The Turing Award, which was introduced in 1966, is often called the Nobel Prize of computing, and it includes a $1 million prize, which the three scientists will share.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tk = sent_tokenize(text)\n",
    "print(\"Sentence tokenizing the text: \\n\")\n",
    "print(sent_tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokenizing the text: \n",
      "\n",
      "['On', 'Wednesday', ',', 'the', 'Association', 'for', 'Computing', 'Machinery', ',', 'the', 'world', '’', 's', 'largest', 'society', 'of', 'computing', 'professionals', ',', 'announced', 'that', 'Hinton', ',', 'LeCun', 'and', 'Bengio', 'had', 'won', 'this', 'year', '’', 's', 'Turing', 'Award', 'for', 'their', 'work', 'on', 'neural', 'networks', '.', 'The', 'Turing', 'Award', ',', 'which', 'was', 'introduced', 'in', '1966', ',', 'is', 'often', 'called', 'the', 'Nobel', 'Prize', 'of', 'computing', ',', 'and', 'it', 'includes', 'a', '$', '1', 'million', 'prize', ',', 'which', 'the', 'three', 'scientists', 'will', 'share', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tk = word_tokenize(text)\n",
    "print(\"Word tokenizing the text: \\n\")\n",
    "print(word_tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wayla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words in English language \n",
      "\n",
      "{'which', 'yourself', \"aren't\", 'don', 'how', 'that', 'we', 'in', 'be', \"you're\", 'was', 'at', 'where', 'for', \"didn't\", 'below', 'yourselves', 'aren', 'each', \"she's\", 'with', 's', 're', 'their', \"should've\", 'whom', 'there', \"don't\", 'didn', 'you', 'doing', 'few', 'won', 've', 'its', 'myself', 'does', 'more', \"haven't\", 'through', \"couldn't\", 'against', \"weren't\", 'and', \"doesn't\", \"wouldn't\", 'some', \"mustn't\", 'most', 'not', 'who', 'y', 'just', 'hadn', 'very', 'ourselves', 'these', 'were', \"wasn't\", 'nor', 'mightn', 'until', 'is', 'him', 'am', 'no', 'than', 'hasn', 'if', 'why', 'our', 'all', 'couldn', 'ain', 'any', 'shan', 'before', 'will', \"hasn't\", \"hadn't\", 'hers', 'them', 'yours', 'having', 'll', 'same', \"needn't\", 'did', 'had', \"shouldn't\", 'so', 'during', 'her', 'a', 'under', 'up', 'do', 'd', 'both', 'about', 'out', 'isn', 'wouldn', 'once', 'too', 'they', 'now', 'between', 'should', 'off', 'further', 'himself', 'again', 'doesn', 'needn', 'themselves', 'to', 'an', 'while', 'being', 'can', 'by', 'are', 'such', \"mightn't\", 'own', 'only', \"shan't\", 'she', 'then', 'other', 'm', 'itself', 'has', 'your', 'my', \"you'd\", \"it's\", 'but', 'mustn', \"won't\", 'he', 'it', 'ma', 'me', 'i', \"isn't\", 'ours', 'weren', 'been', 'here', 'the', 'shouldn', 'have', 'those', 'of', 'down', 'into', \"that'll\", 'theirs', 'on', 'o', 'as', 'after', \"you've\", 'his', 'what', 't', 'above', 'this', 'wasn', \"you'll\", 'or', 'from', 'when', 'herself', 'over', 'haven', 'because'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = set(stopwords.words(\"english\"))\n",
    "print(\"Stop words in English language \\n\")\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On',\n",
       " 'Wednesday',\n",
       " ',',\n",
       " 'Association',\n",
       " 'Computing',\n",
       " 'Machinery',\n",
       " ',',\n",
       " 'world',\n",
       " '’',\n",
       " 'largest',\n",
       " 'society',\n",
       " 'computing',\n",
       " 'professionals',\n",
       " ',',\n",
       " 'announced',\n",
       " 'Hinton',\n",
       " ',',\n",
       " 'LeCun',\n",
       " 'Bengio',\n",
       " 'year',\n",
       " '’',\n",
       " 'Turing',\n",
       " 'Award',\n",
       " 'work',\n",
       " 'neural',\n",
       " 'networks',\n",
       " '.',\n",
       " 'The',\n",
       " 'Turing',\n",
       " 'Award',\n",
       " ',',\n",
       " 'introduced',\n",
       " '1966',\n",
       " ',',\n",
       " 'often',\n",
       " 'called',\n",
       " 'Nobel',\n",
       " 'Prize',\n",
       " 'computing',\n",
       " ',',\n",
       " 'includes',\n",
       " '$',\n",
       " '1',\n",
       " 'million',\n",
       " 'prize',\n",
       " ',',\n",
       " 'three',\n",
       " 'scientists',\n",
       " 'share',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_stops = [w for w in word_tk if not w in sw]\n",
    "filtered_stops #non common english words in the txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stemming</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "port_stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Parts of Speech Tagging</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Frequency Distribution Plots</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
